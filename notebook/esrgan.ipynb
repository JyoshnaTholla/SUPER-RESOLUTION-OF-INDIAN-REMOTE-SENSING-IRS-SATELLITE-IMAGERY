{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98b2c0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-11T17:05:15.062292Z",
     "iopub.status.busy": "2025-02-11T17:05:15.061924Z",
     "iopub.status.idle": "2025-02-11T17:05:28.496865Z",
     "shell.execute_reply": "2025-02-11T17:05:28.496088Z"
    },
    "papermill": {
     "duration": 13.449045,
     "end_time": "2025-02-11T17:05:28.498481",
     "exception": false,
     "start_time": "2025-02-11T17:05:15.049436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras import layers, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras import Model\n",
    "from keras.layers import Conv2D, PReLU,BatchNormalization, Flatten\n",
    "from keras.layers import UpSampling2D, LeakyReLU, Dense, Input, add\n",
    "from tqdm import tqdm\n",
    "from keras.applications import VGG19\n",
    "import random\n",
    "from keras.models import load_model\n",
    "from numpy.random import randint\n",
    "from keras.layers import Lambda, concatenate\n",
    "import tensorflow as tf\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fcf204",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:05:28.509628Z",
     "iopub.status.busy": "2025-02-11T17:05:28.509178Z",
     "iopub.status.idle": "2025-02-11T17:05:29.178917Z",
     "shell.execute_reply": "2025-02-11T17:05:29.178072Z"
    },
    "papermill": {
     "duration": 0.676344,
     "end_time": "2025-02-11T17:05:29.180176",
     "exception": false,
     "start_time": "2025-02-11T17:05:28.503832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----- GPU Setup (for T4x2 or similar) -----\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth to prevent TensorFlow from consuming all GPU memory\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU is available and memory growth is set.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found, running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f343b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:05:29.191138Z",
     "iopub.status.busy": "2025-02-11T17:05:29.190860Z",
     "iopub.status.idle": "2025-02-11T17:05:29.196912Z",
     "shell.execute_reply": "2025-02-11T17:05:29.196101Z"
    },
    "papermill": {
     "duration": 0.012797,
     "end_time": "2025-02-11T17:05:29.198242",
     "exception": false,
     "start_time": "2025-02-11T17:05:29.185445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#RRDB (Residual-in-Residual Dense Block):\n",
    "def rrdb_block(input_tensor, filters=64, gc=32):\n",
    "    # Residual-in-Residual Dense Block\n",
    "    x1 = Conv2D(gc, (3,3), padding='same')(input_tensor)\n",
    "    x1 = LeakyReLU(alpha=0.2)(x1)\n",
    "    x1 = layers.concatenate([input_tensor, x1])\n",
    "    \n",
    "    x2 = Conv2D(gc, (3,3), padding='same')(x1)\n",
    "    x2 = LeakyReLU(alpha=0.2)(x2)\n",
    "    x2 = layers.concatenate([x1, x2])\n",
    "    \n",
    "    x3 = Conv2D(gc, (3,3), padding='same')(x2)\n",
    "    x3 = LeakyReLU(alpha=0.2)(x3)\n",
    "    x3 = layers.concatenate([x2, x3])\n",
    "    \n",
    "    x4 = Conv2D(filters, (3,3), padding='same')(x3)\n",
    "    x4 = Lambda(lambda x: x * 0.2)(x4)  # Residual scaling\n",
    "    out = add([input_tensor, x4])\n",
    "    return out\n",
    "\n",
    "def upscale_block(ip):\n",
    "    x = Conv2D(256, (3,3), padding='same')(ip)\n",
    "    x = UpSampling2D(size=2)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c07981",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:05:29.208888Z",
     "iopub.status.busy": "2025-02-11T17:05:29.208666Z",
     "iopub.status.idle": "2025-02-11T17:05:29.213732Z",
     "shell.execute_reply": "2025-02-11T17:05:29.213143Z"
    },
    "papermill": {
     "duration": 0.0116,
     "end_time": "2025-02-11T17:05:29.214843",
     "exception": false,
     "start_time": "2025-02-11T17:05:29.203243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generator with RRDB blocks\n",
    "def create_gen(gen_ip, num_res_block=16):\n",
    "    x = Conv2D(64, (3,3), padding='same')(gen_ip)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    temp = x\n",
    "    \n",
    "    for _ in range(num_res_block):\n",
    "        x = rrdb_block(x)\n",
    "    \n",
    "    x = Conv2D(64, (3,3), padding='same')(x)\n",
    "    x = add([x, temp])\n",
    "    \n",
    "    x = upscale_block(x)\n",
    "    x = upscale_block(x)\n",
    "    \n",
    "    x = Conv2D(3, (3,3), padding='same', activation='sigmoid')(x) \n",
    "    return Model(gen_ip, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea5c90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:05:29.225457Z",
     "iopub.status.busy": "2025-02-11T17:05:29.225259Z",
     "iopub.status.idle": "2025-02-11T17:05:29.230790Z",
     "shell.execute_reply": "2025-02-11T17:05:29.230218Z"
    },
    "papermill": {
     "duration": 0.012131,
     "end_time": "2025-02-11T17:05:29.231981",
     "exception": false,
     "start_time": "2025-02-11T17:05:29.219850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Relativistic Discriminator\n",
    "def create_disc(disc_ip):\n",
    "    df = 64\n",
    "    x = Conv2D(df, (3,3), padding='same')(disc_ip)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Conv2D(df, (3,3), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Conv2D(df*2, (3,3), padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Conv2D(df*2, (3,3), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Conv2D(df*4, (3,3), padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Conv2D(df*4, (3,3), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Conv2D(df*8, (3,3), padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Conv2D(df*8, (3,3), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(df*16)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dense(1)(x)  # No sigmoid for relativistic loss\n",
    "    \n",
    "    return Model(disc_ip, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06975b58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:05:29.242323Z",
     "iopub.status.busy": "2025-02-11T17:05:29.242121Z",
     "iopub.status.idle": "2025-02-11T17:05:29.245545Z",
     "shell.execute_reply": "2025-02-11T17:05:29.244955Z"
    },
    "papermill": {
     "duration": 0.009714,
     "end_time": "2025-02-11T17:05:29.246589",
     "exception": false,
     "start_time": "2025-02-11T17:05:29.236875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VGG19 for feature extraction\n",
    "from keras.applications import VGG19\n",
    "def build_vgg(hr_shape):\n",
    "    vgg = VGG19(weights=\"imagenet\", include_top=False, input_shape=hr_shape)\n",
    "    return Model(inputs=vgg.inputs, outputs=vgg.layers[10].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370eb59c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:05:29.256853Z",
     "iopub.status.busy": "2025-02-11T17:05:29.256656Z",
     "iopub.status.idle": "2025-02-11T17:05:29.260216Z",
     "shell.execute_reply": "2025-02-11T17:05:29.259581Z"
    },
    "papermill": {
     "duration": 0.009831,
     "end_time": "2025-02-11T17:05:29.261396",
     "exception": false,
     "start_time": "2025-02-11T17:05:29.251565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Combined model\n",
    "def create_comb(gen_model, disc_model, vgg, lr_ip, hr_ip):\n",
    "    gen_img = gen_model(lr_ip)\n",
    "    \n",
    "    gen_features = vgg(gen_img)\n",
    "    \n",
    "    disc_model.trainable = False\n",
    "    validity = disc_model(gen_img)\n",
    "    \n",
    "    return Model(inputs=[lr_ip, hr_ip], outputs=[validity, gen_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0776dca2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:05:29.272055Z",
     "iopub.status.busy": "2025-02-11T17:05:29.271810Z",
     "iopub.status.idle": "2025-02-11T17:06:04.328978Z",
     "shell.execute_reply": "2025-02-11T17:06:04.328275Z"
    },
    "papermill": {
     "duration": 35.064196,
     "end_time": "2025-02-11T17:06:04.330516",
     "exception": false,
     "start_time": "2025-02-11T17:05:29.266320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data loading with your specified paths\n",
    "n = 5000  # Number of images to use\n",
    "lr_path = \"/kaggle/input/sr-data/DATASET/AUG/LR_Aug\"\n",
    "hr_path = \"/kaggle/input/sr-data/DATASET/AUG/HR_Aug\"\n",
    "\n",
    "# Load LR images\n",
    "lr_list = os.listdir(lr_path)[:n]\n",
    "lr_images = []\n",
    "for img in lr_list:\n",
    "    img_path = os.path.join(lr_path, img)\n",
    "    img_lr = cv2.imread(img_path)\n",
    "    img_lr = cv2.cvtColor(img_lr, cv2.COLOR_BGR2RGB)\n",
    "    lr_images.append(img_lr)\n",
    "\n",
    "# Load HR images\n",
    "hr_list = os.listdir(hr_path)[:n]\n",
    "hr_images = []\n",
    "for img in hr_list:\n",
    "    img_path = os.path.join(hr_path, img)\n",
    "    img_hr = cv2.imread(img_path)\n",
    "    img_hr = cv2.cvtColor(img_hr, cv2.COLOR_BGR2RGB)\n",
    "    hr_images.append(img_hr)\n",
    "\n",
    "lr_images = np.array(lr_images) / 255.0\n",
    "hr_images = np.array(hr_images) / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92841232",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:06:04.341768Z",
     "iopub.status.busy": "2025-02-11T17:06:04.341530Z",
     "iopub.status.idle": "2025-02-11T17:06:04.697794Z",
     "shell.execute_reply": "2025-02-11T17:06:04.697028Z"
    },
    "papermill": {
     "duration": 0.366283,
     "end_time": "2025-02-11T17:06:04.702314",
     "exception": false,
     "start_time": "2025-02-11T17:06:04.336031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plotting random images for example\n",
    "image_number = random.randint(0, len(lr_images)-1)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(221)\n",
    "plt.imshow(np.reshape(lr_images[image_number], (64, 64, 3)))\n",
    "plt.subplot(222)\n",
    "plt.imshow(np.reshape(hr_images[image_number], (256, 256, 3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0b8e1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:06:04.721671Z",
     "iopub.status.busy": "2025-02-11T17:06:04.721439Z",
     "iopub.status.idle": "2025-02-11T17:06:06.731206Z",
     "shell.execute_reply": "2025-02-11T17:06:06.730467Z"
    },
    "papermill": {
     "duration": 2.021133,
     "end_time": "2025-02-11T17:06:06.732841",
     "exception": false,
     "start_time": "2025-02-11T17:06:04.711708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert to numpy arrays and normalize\n",
    "lr_images = np.array(lr_images) / 255.0\n",
    "hr_images = np.array(hr_images) / 255.0\n",
    "\n",
    "# Train-test split\n",
    "lr_train, lr_test, hr_train, hr_test = train_test_split(\n",
    "    lr_images, hr_images, test_size=0.33, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6464ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:06:06.752138Z",
     "iopub.status.busy": "2025-02-11T17:06:06.751861Z",
     "iopub.status.idle": "2025-02-11T17:06:09.160709Z",
     "shell.execute_reply": "2025-02-11T17:06:09.159957Z"
    },
    "papermill": {
     "duration": 2.427786,
     "end_time": "2025-02-11T17:06:09.170066",
     "exception": false,
     "start_time": "2025-02-11T17:06:06.742280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hr_shape = (256, 256, 3)\n",
    "lr_shape = (64, 64, 3)\n",
    "\n",
    "# Initialize models\n",
    "generator = create_gen(Input(lr_shape))\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7553c03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:06:09.198842Z",
     "iopub.status.busy": "2025-02-11T17:06:09.198593Z",
     "iopub.status.idle": "2025-02-11T17:06:09.316257Z",
     "shell.execute_reply": "2025-02-11T17:06:09.315478Z"
    },
    "papermill": {
     "duration": 0.133051,
     "end_time": "2025-02-11T17:06:09.317455",
     "exception": false,
     "start_time": "2025-02-11T17:06:09.184404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "discriminator = create_disc(Input(hr_shape))\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b0d90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:06:09.350684Z",
     "iopub.status.busy": "2025-02-11T17:06:09.350385Z",
     "iopub.status.idle": "2025-02-11T17:06:10.207620Z",
     "shell.execute_reply": "2025-02-11T17:06:10.206632Z"
    },
    "papermill": {
     "duration": 0.87525,
     "end_time": "2025-02-11T17:06:10.208870",
     "exception": false,
     "start_time": "2025-02-11T17:06:09.333620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vgg = build_vgg(hr_shape)\n",
    "print(vgg.summary())\n",
    "vgg.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae39efe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:06:10.242521Z",
     "iopub.status.busy": "2025-02-11T17:06:10.242248Z",
     "iopub.status.idle": "2025-02-11T17:06:10.302668Z",
     "shell.execute_reply": "2025-02-11T17:06:10.302030Z"
    },
    "papermill": {
     "duration": 0.078906,
     "end_time": "2025-02-11T17:06:10.304168",
     "exception": false,
     "start_time": "2025-02-11T17:06:10.225262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "gen_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea85aa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:06:10.336650Z",
     "iopub.status.busy": "2025-02-11T17:06:10.336382Z",
     "iopub.status.idle": "2025-02-11T17:06:14.397455Z",
     "shell.execute_reply": "2025-02-11T17:06:14.396759Z"
    },
    "papermill": {
     "duration": 4.078533,
     "end_time": "2025-02-11T17:06:14.398945",
     "exception": false,
     "start_time": "2025-02-11T17:06:10.320412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 1\n",
    "epochs = 25\n",
    "# Prepare dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((lr_train, hr_train)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b536c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:06:14.432845Z",
     "iopub.status.busy": "2025-02-11T17:06:14.432571Z",
     "iopub.status.idle": "2025-02-11T17:06:14.435631Z",
     "shell.execute_reply": "2025-02-11T17:06:14.435063Z"
    },
    "papermill": {
     "duration": 0.021493,
     "end_time": "2025-02-11T17:06:14.436875",
     "exception": false,
     "start_time": "2025-02-11T17:06:14.415382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize loss tracking (before training loop)\n",
    "gen_losses = []\n",
    "disc_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea15111e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T17:06:14.469310Z",
     "iopub.status.busy": "2025-02-11T17:06:14.469072Z",
     "iopub.status.idle": "2025-02-12T02:10:19.501204Z",
     "shell.execute_reply": "2025-02-12T02:10:19.500542Z"
    },
    "papermill": {
     "duration": 32645.0504,
     "end_time": "2025-02-12T02:10:19.502605",
     "exception": false,
     "start_time": "2025-02-11T17:06:14.452205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----- Training Loop -----\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    epoch_gen_loss = []\n",
    "    epoch_disc_loss = []\n",
    "    \n",
    "    for step, (lr_batch, hr_batch) in enumerate(tqdm(train_dataset)):\n",
    "        # --- Train Discriminator ---\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            fake_hr = generator(lr_batch, training=True)\n",
    "            real_output = discriminator(hr_batch, training=True)\n",
    "            fake_output = discriminator(fake_hr, training=True)\n",
    "            \n",
    "            # Relativistic discriminator loss\n",
    "            real_loss = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.ones_like(real_output),\n",
    "                    logits=real_output - tf.reduce_mean(fake_output)\n",
    "                )\n",
    "            )\n",
    "            fake_loss = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.zeros_like(fake_output),\n",
    "                    logits=fake_output - tf.reduce_mean(real_output)\n",
    "                )\n",
    "            )\n",
    "            disc_loss = (real_loss + fake_loss) * 0.5\n",
    "        \n",
    "        disc_grads = disc_tape.gradient(disc_loss, discriminator.trainable_weights)\n",
    "        disc_optimizer.apply_gradients(zip(disc_grads, discriminator.trainable_weights))\n",
    "        epoch_disc_loss.append(disc_loss.numpy())\n",
    "        \n",
    "        # --- Train Generator ---\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            fake_hr = generator(lr_batch, training=True)\n",
    "            \n",
    "            fake_output = discriminator(fake_hr, training=False)\n",
    "            real_output = discriminator(hr_batch, training=False)\n",
    "            gen_adv_loss = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.ones_like(fake_output),\n",
    "                    logits=fake_output - tf.reduce_mean(real_output)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Feature (content) loss using VGG19\n",
    "            gen_features = vgg(fake_hr)\n",
    "            real_features = vgg(hr_batch)\n",
    "            gen_content_loss = tf.keras.losses.MeanSquaredError()(real_features, gen_features)\n",
    "            \n",
    "            # Total generator loss (adjust weight of adversarial loss as needed)\n",
    "            gen_total_loss = gen_content_loss + 1e-3 * gen_adv_loss\n",
    "        \n",
    "        gen_grads = gen_tape.gradient(gen_total_loss, generator.trainable_weights)\n",
    "        gen_optimizer.apply_gradients(zip(gen_grads, generator.trainable_weights))\n",
    "        epoch_gen_loss.append(gen_total_loss.numpy())\n",
    "        \n",
    "    avg_gen_loss = np.mean(epoch_gen_loss)\n",
    "    avg_disc_loss = np.mean(epoch_disc_loss)\n",
    "    gen_losses.append(avg_gen_loss)\n",
    "    disc_losses.append(avg_disc_loss)\n",
    "    print(f\"Epoch {epoch+1}: Generator Loss = {avg_gen_loss:.4f}, Discriminator Loss = {avg_disc_loss:.4f}\")\n",
    "    \n",
    "    # Save model periodically (here every epoch, adjust as needed)\n",
    "    generator.save(f\"esrgan_gen_epoch{epoch+1}.h5\")\n",
    "\n",
    "generator.save(\"esrgan_generator_final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc665d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T02:10:21.942066Z",
     "iopub.status.busy": "2025-02-12T02:10:21.941732Z",
     "iopub.status.idle": "2025-02-12T02:10:22.116043Z",
     "shell.execute_reply": "2025-02-12T02:10:22.115122Z"
    },
    "papermill": {
     "duration": 1.328877,
     "end_time": "2025-02-12T02:10:22.117705",
     "exception": false,
     "start_time": "2025-02-12T02:10:20.788828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator.save(\"esrgan_generator_final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aaa361",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T02:10:24.569068Z",
     "iopub.status.busy": "2025-02-12T02:10:24.568702Z",
     "iopub.status.idle": "2025-02-12T02:10:24.572758Z",
     "shell.execute_reply": "2025-02-12T02:10:24.571779Z"
    },
    "papermill": {
     "duration": 1.157224,
     "end_time": "2025-02-12T02:10:24.574271",
     "exception": false,
     "start_time": "2025-02-12T02:10:23.417047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----Visualisations and Grpahs----#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a1e058",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T02:10:27.120585Z",
     "iopub.status.busy": "2025-02-12T02:10:27.120274Z",
     "iopub.status.idle": "2025-02-12T02:10:52.866021Z",
     "shell.execute_reply": "2025-02-12T02:10:52.865267Z"
    },
    "papermill": {
     "duration": 27.049324,
     "end_time": "2025-02-12T02:10:52.867542",
     "exception": false,
     "start_time": "2025-02-12T02:10:25.818218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "def evaluate_gan(generator, lr_test, hr_test, batch_size=8):\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    mse_values = []\n",
    "    lr_samples = []\n",
    "    hr_samples = []\n",
    "    sr_samples = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(lr_test), batch_size), desc=\"Generating SR images\"):\n",
    "        lr_batch = lr_test[i:i+batch_size]\n",
    "        hr_batch = hr_test[i:i+batch_size]\n",
    "        \n",
    "        sr_batch = generator.predict(lr_batch, verbose=0)\n",
    "        sr_batch = np.clip(sr_batch, 0.0, 1.0)  # Clip outputs to [0, 1]\n",
    "        \n",
    "        for j in range(len(hr_batch)):\n",
    "            psnr_val = psnr(hr_batch[j], sr_batch[j], data_range=1.0)\n",
    "            ssim_val = ssim(hr_batch[j], sr_batch[j], data_range=1.0, channel_axis=-1)\n",
    "            mse_val = np.mean((hr_batch[j] - sr_batch[j]) ** 2)\n",
    "            \n",
    "            psnr_values.append(psnr_val)\n",
    "            ssim_values.append(ssim_val)\n",
    "            mse_values.append(mse_val)\n",
    "            \n",
    "        lr_samples.extend(lr_batch)\n",
    "        hr_samples.extend(hr_batch)\n",
    "        sr_samples.extend(sr_batch)\n",
    "    \n",
    "    return {\n",
    "        'psnr': psnr_values,\n",
    "        'ssim': ssim_values,\n",
    "        'mse': mse_values,\n",
    "        'lr_samples': np.array(lr_samples),\n",
    "        'hr_samples': np.array(hr_samples),\n",
    "        'sr_samples': np.array(sr_samples)\n",
    "    }\n",
    "gan_metrics = evaluate_gan(generator, lr_test, hr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94787561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T02:10:55.299326Z",
     "iopub.status.busy": "2025-02-12T02:10:55.299020Z",
     "iopub.status.idle": "2025-02-12T02:10:55.511090Z",
     "shell.execute_reply": "2025-02-12T02:10:55.510263Z"
    },
    "papermill": {
     "duration": 1.496606,
     "end_time": "2025-02-12T02:10:55.512537",
     "exception": false,
     "start_time": "2025-02-12T02:10:54.015931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss Visualization \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(gen_losses, label='Training Loss')\n",
    "plt.plot(disc_losses, label='Validation Loss')\n",
    "plt.title('ESRGAN Training Progress')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8bcd88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T02:10:57.958440Z",
     "iopub.status.busy": "2025-02-12T02:10:57.958151Z",
     "iopub.status.idle": "2025-02-12T02:10:58.647864Z",
     "shell.execute_reply": "2025-02-12T02:10:58.646971Z"
    },
    "papermill": {
     "duration": 1.963533,
     "end_time": "2025-02-12T02:10:58.649223",
     "exception": false,
     "start_time": "2025-02-12T02:10:56.685690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Metric Distributions\n",
    "def plot_gan_metrics(gan_metrics):\n",
    "    \"\"\"Visualize distribution of GAN evaluation metrics\"\"\"\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18,5))\n",
    "    \n",
    "    metrics = ['psnr', 'ssim', 'mse']\n",
    "    titles = ['PSNR Distribution', 'SSIM Distribution', 'MSE Distribution']\n",
    "    colors = ['purple', 'green', 'red']\n",
    "    \n",
    "    for ax, metric, title, color in zip(axs, metrics, titles, colors):\n",
    "        ax.hist(gan_metrics[metric], bins=30, alpha=0.7, color=color)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(metric.upper())\n",
    "        ax.set_ylabel('Frequency')\n",
    "        \n",
    "        # Add vertical lines for mean values\n",
    "        mean_val = np.mean(gan_metrics[metric])\n",
    "        ax.axvline(mean_val, color='black', linestyle='dashed', linewidth=2,\n",
    "                  label=f'Mean: {mean_val:.2f}')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_gan_metrics(gan_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbdbf4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T02:11:01.116723Z",
     "iopub.status.busy": "2025-02-12T02:11:01.116382Z",
     "iopub.status.idle": "2025-02-12T02:11:05.388449Z",
     "shell.execute_reply": "2025-02-12T02:11:05.387655Z"
    },
    "papermill": {
     "duration": 5.546392,
     "end_time": "2025-02-12T02:11:05.390399",
     "exception": false,
     "start_time": "2025-02-12T02:10:59.844007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample Comparisions\n",
    "def plot_gan_samples(gan_metrics, num_samples=10):\n",
    "    \"\"\"Visual comparison with error heatmaps\"\"\"\n",
    "    indices = np.random.choice(len(gan_metrics['lr_samples']), num_samples)\n",
    "    \n",
    "    for idx in indices:\n",
    "        plt.figure(figsize=(18,4))\n",
    "        \n",
    "        # LR Input\n",
    "        plt.subplot(141)\n",
    "        plt.imshow(gan_metrics['lr_samples'][idx])\n",
    "        plt.title('Medium-Resolution Input')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # HR Ground Truth\n",
    "        plt.subplot(142)\n",
    "        plt.imshow(gan_metrics['hr_samples'][idx])\n",
    "        plt.title('High-Resolution Target')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # SR Output\n",
    "        plt.subplot(143)\n",
    "        plt.imshow(gan_metrics['sr_samples'][idx])\n",
    "        metrics_text = f\"PSNR: {gan_metrics['psnr'][idx]:.2f}\\n\" \\\n",
    "                       f\"SSIM: {gan_metrics['ssim'][idx]:.4f}\\n\" \\\n",
    "                       f\"MSE: {gan_metrics['mse'][idx]:.5f}\"\n",
    "        plt.title('ESRGAN Output\\n' + metrics_text)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Error Map\n",
    "        plt.subplot(144)\n",
    "        error = np.abs(gan_metrics['hr_samples'][idx] - gan_metrics['sr_samples'][idx])\n",
    "        plt.imshow(np.mean(error, axis=-1), cmap='inferno', vmin=0, vmax=0.07)  # Set dynamic range\n",
    "        plt.title('Pixel Error Heatmap')\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "plot_gan_samples(gan_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4258c277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T02:11:07.890041Z",
     "iopub.status.busy": "2025-02-12T02:11:07.889676Z",
     "iopub.status.idle": "2025-02-12T02:11:11.138245Z",
     "shell.execute_reply": "2025-02-12T02:11:11.137347Z"
    },
    "papermill": {
     "duration": 4.564893,
     "end_time": "2025-02-12T02:11:11.139747",
     "exception": false,
     "start_time": "2025-02-12T02:11:06.574854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Error Analysis\n",
    "def plot_gan_error_analysis(gan_metrics):\n",
    "    \"\"\"Detailed error characterization\"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12,5))\n",
    "    \n",
    "    # Pixel Error Distribution\n",
    "    errors = np.concatenate([np.abs(hr - sr) for hr, sr in \n",
    "                           zip(gan_metrics['hr_samples'], gan_metrics['sr_samples'])])\n",
    "    axs[0].hist(errors.flatten(), bins=50, color='darkorange', density=True)\n",
    "    axs[0].set_title('Pixel Error Distribution')\n",
    "    axs[0].set_xlabel('Absolute Error')\n",
    "    axs[0].set_ylabel('Density')\n",
    "    \n",
    "    # Metric Correlation\n",
    "    axs[1].scatter(gan_metrics['psnr'], gan_metrics['ssim'], alpha=0.5)\n",
    "    axs[1].set_title('PSNR vs SSIM Correlation')\n",
    "    axs[1].set_xlabel('PSNR')\n",
    "    axs[1].set_ylabel('SSIM')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_gan_error_analysis(gan_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4fd3b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T02:11:13.676487Z",
     "iopub.status.busy": "2025-02-12T02:11:13.676157Z",
     "iopub.status.idle": "2025-02-12T02:11:58.317366Z",
     "shell.execute_reply": "2025-02-12T02:11:58.316347Z"
    },
    "papermill": {
     "duration": 46.011946,
     "end_time": "2025-02-12T02:11:58.318735",
     "exception": false,
     "start_time": "2025-02-12T02:11:12.306789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----Calculate the average metrics------#\n",
    "#PSNR: Higher is better\n",
    "#SSIM: Closer to 1 is better\n",
    "#MSE: Lower is better\n",
    "\n",
    "def evaluate_esrgan(generator, lr_test, hr_test):\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    mse_values = []\n",
    "    \n",
    "    for i in tqdm(range(len(lr_test))):\n",
    "        lr = lr_test[i:i+1]\n",
    "        hr = hr_test[i:i+1]\n",
    "        \n",
    "        gen_img = generator.predict(lr)\n",
    "        gen_img = np.clip(gen_img, 0, 1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        psnr_val = psnr(hr[0], gen_img[0], data_range=1.0)\n",
    "        ssim_val = ssim(hr[0], gen_img[0], \n",
    "                       data_range=1.0, multichannel=True, channel_axis=-1)\n",
    "        mse = np.mean((hr[0] - gen_img[0]) ** 2)\n",
    "        \n",
    "        psnr_values.append(psnr_val)\n",
    "        ssim_values.append(ssim_val)\n",
    "        mse_values.append(mse)\n",
    "    \n",
    "    metrics_df = pd.DataFrame({\n",
    "        'PSNR': psnr_values,\n",
    "        'SSIM': ssim_values,\n",
    "        'MSE': mse_values\n",
    "    })\n",
    "    \n",
    "    print(\"\\nAverage Metrics:\")\n",
    "    print(metrics_df.mean())\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "esrgan_metrics = evaluate_esrgan(generator, lr_test, hr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d420a424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T02:12:00.995581Z",
     "iopub.status.busy": "2025-02-12T02:12:00.995047Z",
     "iopub.status.idle": "2025-02-12T02:12:01.326689Z",
     "shell.execute_reply": "2025-02-12T02:12:01.325541Z"
    },
    "papermill": {
     "duration": 1.56656,
     "end_time": "2025-02-12T02:12:01.327816",
     "exception": true,
     "start_time": "2025-02-12T02:11:59.761256",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# ======================== Load Dataset ========================\n",
    "def load_new_srgan_esrgan_dataset(new_lr_path, new_hr_path, batch_size=4, n=None):\n",
    "    \"\"\"Load new dataset for SRGAN/ESRGAN testing and create TensorFlow dataset\"\"\"\n",
    "    new_lr_list = os.listdir(new_lr_path)\n",
    "    new_hr_list = os.listdir(new_hr_path)\n",
    "    \n",
    "    if n is not None:\n",
    "        new_lr_list = new_lr_list[:n]\n",
    "        new_hr_list = new_hr_list[:n]\n",
    "    \n",
    "    # Load new LR images\n",
    "    new_lr_images = []\n",
    "    for img in tqdm(new_lr_list, desc=\"Loading New LR Images\"):\n",
    "        img_path = os.path.join(new_lr_path, img)\n",
    "        img_lr = cv2.imread(img_path)\n",
    "        img_lr = cv2.cvtColor(img_lr, cv2.COLOR_BGR2RGB)\n",
    "        new_lr_images.append(img_lr)\n",
    "    \n",
    "    # Load new HR images\n",
    "    new_hr_images = []\n",
    "    for img in tqdm(new_hr_list, desc=\"Loading New HR Images\"):\n",
    "        img_path = os.path.join(new_hr_path, img)\n",
    "        img_hr = cv2.imread(img_path)\n",
    "        img_hr = cv2.cvtColor(img_hr, cv2.COLOR_BGR2RGB)\n",
    "        new_hr_images.append(img_hr)\n",
    "    \n",
    "    # Convert to numpy arrays and normalize\n",
    "    new_lr_images = np.array(new_lr_images) / 255.0\n",
    "    new_hr_images = np.array(new_hr_images) / 255.0\n",
    "    \n",
    "    # Create TensorFlow dataset\n",
    "    new_dataset = tf.data.Dataset.from_tensor_slices((new_lr_images, new_hr_images))\n",
    "    new_dataset = new_dataset.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    return new_dataset, new_lr_images, new_hr_images\n",
    "\n",
    "# ======================== Model Evaluation =======================\n",
    "def evaluate_srgan_or_esrgan(generator, discriminator, dataset):\n",
    "    \"\"\"Evaluate SRGAN/ESRGAN on the given dataset\"\"\"\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    mse_values = []\n",
    "    gan_loss_values = []  # For GAN loss evaluation\n",
    "\n",
    "    for lr_image, hr_image in dataset:\n",
    "        # Generate high-resolution image using the generator\n",
    "        sr_image = generator.predict(lr_image)\n",
    "        \n",
    "        # Calculate traditional metrics\n",
    "        psnr = calculate_psnr(hr_image, sr_image)\n",
    "        ssim = calculate_ssim(hr_image, sr_image)\n",
    "        mse = calculate_mse(hr_image, sr_image)\n",
    "\n",
    "        # Calculate GAN loss using discriminator\n",
    "        gan_loss = discriminator.evaluate(lr_image, sr_image)\n",
    "        \n",
    "        psnr_values.append(psnr)\n",
    "        ssim_values.append(ssim)\n",
    "        mse_values.append(mse)\n",
    "        gan_loss_values.append(gan_loss)\n",
    "    \n",
    "    metrics = {\n",
    "        'psnr_values': psnr_values,\n",
    "        'ssim_values': ssim_values,\n",
    "        'mse_values': mse_values,\n",
    "        'gan_loss_values': gan_loss_values\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# ======================== Metric Calculation =======================\n",
    "def calculate_psnr(hr_image, sr_image):\n",
    "    \"\"\"Calculate PSNR between high-resolution and generated images\"\"\"\n",
    "    return tf.image.psnr(hr_image, sr_image, max_val=1.0)\n",
    "\n",
    "def calculate_ssim(hr_image, sr_image):\n",
    "    \"\"\"Calculate SSIM between high-resolution and generated images\"\"\"\n",
    "    return tf.image.ssim(hr_image, sr_image, max_val=1.0)\n",
    "\n",
    "def calculate_mse(hr_image, sr_image):\n",
    "    \"\"\"Calculate MSE between high-resolution and generated images\"\"\"\n",
    "    return tf.reduce_mean(tf.square(hr_image - sr_image))\n",
    "\n",
    "# ======================== Comparing Datasets ========================\n",
    "def compare_esrgan_datasets(original_metrics, new_metrics):\n",
    "    \"\"\"Compare metrics between original test set and new dataset for SRGAN/ESRGAN\"\"\"\n",
    "    comparison = {\n",
    "        'PSNR': {\n",
    "            'Original': np.mean(original_metrics['psnr_values']),\n",
    "            'New Dataset': np.mean(new_metrics['psnr_values'])\n",
    "        },\n",
    "        'SSIM': {\n",
    "            'Original': np.mean(original_metrics['ssim_values']),\n",
    "            'New Dataset': np.mean(new_metrics['ssim_values'])\n",
    "        },\n",
    "        'MSE': {\n",
    "            'Original': np.mean(original_metrics['mse_values']),\n",
    "            'New Dataset': np.mean(new_metrics['mse_values'])\n",
    "        },\n",
    "        'GAN Loss': {\n",
    "            'Original': np.mean(original_metrics['gan_loss_values']),\n",
    "            'New Dataset': np.mean(new_metrics['gan_loss_values'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(comparison).T\n",
    "    print(\"\\nESRGAN Performance Comparison:\")\n",
    "    print(df)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    metrics = ['PSNR', 'SSIM', 'MSE', 'GAN Loss']\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(1, 4, i+1)\n",
    "        plt.bar(['Original', 'New Dataset'], \n",
    "                [comparison[metric]['Original'], comparison[metric]['New Dataset']],\n",
    "                color=['blue', 'orange'])\n",
    "        plt.title(f'{metric} Comparison')\n",
    "        plt.ylabel(metric)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ======================== Visual Comparison =======================\n",
    "def plot_esrgan_side_by_side(original_metrics, new_metrics, num_samples=3):\n",
    "    \"\"\"Visual comparison between original test and new dataset results\"\"\"\n",
    "    orig_indices = np.random.choice(len(original_metrics['psnr_values']), num_samples)\n",
    "    new_indices = np.random.choice(len(new_metrics['psnr_values']), num_samples)\n",
    "    \n",
    "    for orig_idx, new_idx in zip(orig_indices, new_indices):\n",
    "        plt.figure(figsize=(18, 8))\n",
    "        \n",
    "        # Original Test Set Results\n",
    "        plt.subplot(231)\n",
    "        plt.imshow(original_metrics['lr_samples'][orig_idx])\n",
    "        plt.title('Medium-Resolution Input')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(232)\n",
    "        plt.imshow(original_metrics['hr_samples'][orig_idx])\n",
    "        plt.title('High-resolution Target')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(233)\n",
    "        plt.imshow(original_metrics['sr_samples'][orig_idx])\n",
    "        plt.title(f'ESRGAN Output \\nPSNR: {original_metrics[\"psnr_values\"][orig_idx]:.2f}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # New Dataset Results\n",
    "        plt.subplot(234)\n",
    "        plt.imshow(new_metrics['lr_samples'][new_idx])\n",
    "        plt.title('Test MR')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(235)\n",
    "        plt.imshow(new_metrics['hr_samples'][new_idx])\n",
    "        plt.title('Test HR')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(236)\n",
    "        plt.imshow(new_metrics['sr_samples'][new_idx])\n",
    "        plt.title(\"ESRGAN Output\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ======================== USAGE EXAMPLE ========================\n",
    "\n",
    "# Load new dataset (replace paths with your actual paths)\n",
    "new_lr_path = \"/kaggle/input/test-sr/testing dataset/LR\"\n",
    "new_hr_path = \"/kaggle/input/test-sr/testing dataset/HR\"\n",
    "new_test_dataset, new_lr_array, new_hr_array = load_new_esrgan_dataset(new_lr_path, new_hr_path)\n",
    "\n",
    "# Assuming you have a pre-trained SRGAN or ESRGAN generator and discriminator\n",
    "generator = load_srgan_or_esrgan_generator()  # Replace with actual model loading code\n",
    "discriminator = load_srgan_or_esrgan_discriminator()  # Replace with actual model loading code\n",
    "\n",
    "# Evaluate on new dataset\n",
    "new_esrgan_metrics = evaluate_esrgan(generator, discriminator, new_test_dataset)\n",
    "\n",
    "# Compare with original test results\n",
    "compare_esrgan_datasets(original_esrgan_metrics, new_esrgan_metrics)\n",
    "\n",
    "# Visual comparison between test sets\n",
    "plot_esrgan_side_by_side(original_esrgan_metrics, new_esrgan_metrics)\n",
    "\n",
    "# Save comparison results to CSV\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Dataset': ['Original', 'New'] * len(original_srgan_metrics['psnr_values']),\n",
    "    'PSNR': original_srgan_metrics['psnr_values'] + new_srgan_metrics['psnr_values'],\n",
    "    'SSIM': original_srgan_metrics['ssim_values'] + new_srgan_metrics['ssim_values'],\n",
    "    'MSE': original_srgan_metrics['mse_values'] + new_srgan_metrics['mse_values'],\n",
    "    'GAN Loss': original_srgan_metrics['gan_loss_values'] + new_srgan_metrics['gan_loss_values']\n",
    "})\n",
    "comparison_df.to_csv('esrgan_dataset_comparison.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816685ed",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----Test Code for SR predictions----#\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from keras.models import load_model\n",
    "from numpy.random import randint\n",
    "\n",
    "# Load generator model\n",
    "generator = load_model('esrgan_generator_final.h5', compile=False)\n",
    "\n",
    "# Get test data\n",
    "[X1, X2] = [lr_test, hr_test]\n",
    "\n",
    "# Select random example\n",
    "ix = randint(0, len(X1), 1)\n",
    "src_image, tar_image = X1[ix], X2[ix]\n",
    "\n",
    "# Generate super-resolved image\n",
    "gen_image = generator.predict(src_image)\n",
    "\n",
    "\n",
    "# Clip values to valid image range\n",
    "src_image = np.clip(src_image, 0, 1)\n",
    "gen_image = np.clip(gen_image, 0, 1)\n",
    "tar_image = np.clip(tar_image, 0, 1)\n",
    "\n",
    "# Calculate metrics\n",
    "psnr_val = psnr(tar_image[0], gen_image[0], data_range=1.0)\n",
    "ssim_val = ssim(tar_image[0], gen_image[0], \n",
    "                data_range=1.0, multichannel=True, channel_axis=-1)\n",
    "mse = np.mean((tar_image[0] - gen_image[0]) ** 2)\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot LR image\n",
    "plt.subplot(231)\n",
    "plt.imshow(src_image[0])\n",
    "plt.title('Low-Resolution Input')\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot Super-Resolved image\n",
    "plt.subplot(232)\n",
    "plt.imshow(gen_image[0])\n",
    "plt.title(f'ESRGAN Output\\nPSNR: {psnr_val:.2f} dB, SSIM: {ssim_val:.4f}\\nMSE: {mse:.4f}')\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot HR Ground Truth\n",
    "plt.subplot(233)\n",
    "plt.imshow(tar_image[0])\n",
    "plt.title('High-Resolution Ground Truth')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb93311",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----Testing with New Dataset---#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c004451",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================== IMPORTS & SETUP ======================\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# ====================== CONFIGURATION ======================\n",
    "# Update these paths with your actual dataset locations\n",
    "DATASET_PATHS = {\n",
    "    'new_dataset': {\n",
    "        'lr': \"/kaggle/input/test-sr/testing dataset/LR\",\n",
    "        'hr': \"/kaggle/input/test-sr/testing dataset/HR\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ====================== CORE FUNCTIONS ======================\n",
    "def load_dataset(lr_path, hr_path, n=None):\n",
    "    \"\"\"Load dataset from specified paths\"\"\"\n",
    "    lr_images, hr_images = [], []\n",
    "    \n",
    "    # Load LR images\n",
    "    for img in tqdm(sorted(os.listdir(lr_path))[:n], desc=f\"Loading LR from {lr_path}\"):\n",
    "        img_array = cv2.imread(os.path.join(lr_path, img))\n",
    "        lr_images.append(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Load HR images\n",
    "    for img in tqdm(sorted(os.listdir(hr_path))[:n], desc=f\"Loading HR from {hr_path}\"):\n",
    "        img_array = cv2.imread(os.path.join(hr_path, img))\n",
    "        hr_images.append(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    return np.array(lr_images)/255.0, np.array(hr_images)/255.0\n",
    "\n",
    "def evaluate_model(generator, lr_images, hr_images, batch_size=8):\n",
    "    \"\"\"Evaluate model performance on given dataset\"\"\"\n",
    "    metrics = {\n",
    "        'psnr': [], 'ssim': [], 'mse': [],\n",
    "        'lr_samples': [], 'hr_samples': [], 'sr_samples': []\n",
    "    }\n",
    "    \n",
    "    for i in tqdm(range(0, len(lr_images), batch_size), desc=\"Generating SR images\"):\n",
    "        lr_batch = lr_images[i:i+batch_size]\n",
    "        hr_batch = hr_images[i:i+batch_size]\n",
    "        sr_batch = generator.predict(lr_batch, verbose=0)\n",
    "        \n",
    "        for j in range(len(hr_batch)):\n",
    "            metrics['psnr'].append(psnr(hr_batch[j], sr_batch[j], data_range=1.0))\n",
    "            metrics['ssim'].append(ssim(hr_batch[j], sr_batch[j], \n",
    "                                      data_range=1.0, channel_axis=-1))\n",
    "            metrics['mse'].append(np.mean((hr_batch[j] - sr_batch[j]) ** 2))\n",
    "        \n",
    "        metrics['lr_samples'].extend(lr_batch)\n",
    "        metrics['hr_samples'].extend(hr_batch)\n",
    "        metrics['sr_samples'].extend(sr_batch)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# ====================== ANALYSIS & VISUALIZATION ======================\n",
    "def plot_esrgan_samples(esrgan_metrics, num_samples=10):\n",
    "    \"\"\"Visual comparison with error heatmaps (ESRGAN version)\"\"\"\n",
    "    indices = np.random.choice(len(esrgan_metrics['hr_samples']), num_samples)\n",
    "    \n",
    "    for idx in indices:\n",
    "        plt.figure(figsize=(18, 4))\n",
    "        \n",
    "        # LR Input\n",
    "        plt.subplot(141)\n",
    "        plt.imshow(esrgan_metrics['lr_samples'][idx])\n",
    "        plt.title('Medium-Resolution Input')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # HR Ground Truth\n",
    "        plt.subplot(142)\n",
    "        plt.imshow(esrgan_metrics['hr_samples'][idx])\n",
    "        plt.title('High-Resolution Target')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # SR Output\n",
    "        plt.subplot(143)\n",
    "        plt.imshow(esrgan_metrics['sr_samples'][idx])\n",
    "        metrics_text = f\"PSNR: {esrgan_metrics['psnr'][idx]:.2f}\\n\" \\\n",
    "                       f\"SSIM: {esrgan_metrics['ssim'][idx]:.4f}\\n\" \\\n",
    "                       f\"MSE: {esrgan_metrics['mse'][idx]:.5f}\"\n",
    "        plt.title('ESRGAN Output\\n' + metrics_text)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Error Map\n",
    "        plt.subplot(144)\n",
    "        error = np.abs(esrgan_metrics['hr_samples'][idx] - esrgan_metrics['sr_samples'][idx])\n",
    "        plt.imshow(np.mean(error, axis=-1), cmap='inferno')\n",
    "        plt.title('Pixel Error Heatmap')\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ====================== EXECUTION FLOW ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load new evaluation dataset (corrected)\n",
    "    print(\"Loading new evaluation dataset...\")\n",
    "    new_lr, new_hr = load_dataset(DATASET_PATHS['new_dataset']['lr'],\n",
    "                                   DATASET_PATHS['new_dataset']['hr'])\n",
    "    \n",
    "    # 2. Evaluate ESRGAN model on dataset\n",
    "    print(\"\\nEvaluating on new dataset...\")\n",
    "    esrgan_metrics = evaluate_model(generator, new_lr, new_hr)\n",
    "    \n",
    "    # 3. Visual comparison with ESRGAN\n",
    "    print(\"\\nGenerating ESRGAN visual comparisons...\")\n",
    "    plot_esrgan_samples(esrgan_metrics)\n",
    "    \n",
    "    # 4. Save results\n",
    "    esrgan_metrics_df = pd.DataFrame({\n",
    "        'PSNR': esrgan_metrics['psnr'],\n",
    "        'SSIM': esrgan_metrics['ssim'],\n",
    "        'MSE': esrgan_metrics['mse']\n",
    "    })\n",
    "    esrgan_metrics_df.to_csv('esrgan_comparison.csv', index=False)\n",
    "    print(\"\\nComparison saved to esrgan_comparison.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feef19d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================== IMPORTS & SETUP ======================\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# ====================== CONFIGURATION ======================\n",
    "# Update these paths with your actual dataset locations\n",
    "DATASET_PATHS = {\n",
    "    'new_dataset': {\n",
    "        'lr': \"/kaggle/input/sr-data/DATASET/AUG/LR_Aug\",\n",
    "        'hr': \"/kaggle/input/sr-data/DATASET/AUG/HR_Aug\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ====================== CORE FUNCTIONS ======================\n",
    "def load_dataset(lr_path, hr_path, n=None):\n",
    "    \"\"\"Load dataset from specified paths\"\"\"\n",
    "    lr_images, hr_images = [], []\n",
    "    \n",
    "    # Load LR images\n",
    "    for img in tqdm(sorted(os.listdir(lr_path))[:n], desc=f\"Loading LR from {lr_path}\"):\n",
    "        img_array = cv2.imread(os.path.join(lr_path, img))\n",
    "        lr_images.append(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Load HR images\n",
    "    for img in tqdm(sorted(os.listdir(hr_path))[:n], desc=f\"Loading HR from {hr_path}\"):\n",
    "        img_array = cv2.imread(os.path.join(hr_path, img))\n",
    "        hr_images.append(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    return np.array(lr_images)/255.0, np.array(hr_images)/255.0\n",
    "\n",
    "def evaluate_model(generator, lr_images, hr_images, batch_size=8):\n",
    "    \"\"\"Evaluate model performance on given dataset\"\"\"\n",
    "    metrics = {\n",
    "        'psnr': [], 'ssim': [], 'mse': [],\n",
    "        'lr_samples': [], 'hr_samples': [], 'sr_samples': []\n",
    "    }\n",
    "    \n",
    "    for i in tqdm(range(0, len(lr_images), batch_size), desc=\"Generating SR images\"):\n",
    "        lr_batch = lr_images[i:i+batch_size]\n",
    "        hr_batch = hr_images[i:i+batch_size]\n",
    "        sr_batch = generator.predict(lr_batch, verbose=0)\n",
    "        \n",
    "        for j in range(len(hr_batch)):\n",
    "            metrics['psnr'].append(psnr(hr_batch[j], sr_batch[j], data_range=1.0))\n",
    "            metrics['ssim'].append(ssim(hr_batch[j], sr_batch[j], \n",
    "                                      data_range=1.0, channel_axis=-1))\n",
    "            metrics['mse'].append(np.mean((hr_batch[j] - sr_batch[j]) ** 2))\n",
    "        \n",
    "        metrics['lr_samples'].extend(lr_batch)\n",
    "        metrics['hr_samples'].extend(hr_batch)\n",
    "        metrics['sr_samples'].extend(sr_batch)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# ====================== ANALYSIS & VISUALIZATION ======================\n",
    "def plot_esrgan_samples(esrgan_metrics, num_samples=10):\n",
    "    \"\"\"Visual comparison with error heatmaps (ESRGAN version)\"\"\"\n",
    "    indices = np.random.choice(len(esrgan_metrics['hr_samples']), num_samples)\n",
    "    \n",
    "    for idx in indices:\n",
    "        plt.figure(figsize=(18, 4))\n",
    "        \n",
    "        # LR Input\n",
    "        plt.subplot(141)\n",
    "        plt.imshow(esrgan_metrics['lr_samples'][idx])\n",
    "        plt.title('Medium-Resolution Input')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # HR Ground Truth\n",
    "        plt.subplot(142)\n",
    "        plt.imshow(esrgan_metrics['hr_samples'][idx])\n",
    "        plt.title('High-Resolution Target')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # SR Output\n",
    "        plt.subplot(143)\n",
    "        plt.imshow(esrgan_metrics['sr_samples'][idx])\n",
    "        metrics_text = f\"PSNR: {esrgan_metrics['psnr'][idx]:.2f}\\n\" \\\n",
    "                       f\"SSIM: {esrgan_metrics['ssim'][idx]:.4f}\\n\" \\\n",
    "                       f\"MSE: {esrgan_metrics['mse'][idx]:.5f}\"\n",
    "        plt.title('ESRGAN Output\\n' + metrics_text)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Error Map\n",
    "        plt.subplot(144)\n",
    "        error = np.abs(esrgan_metrics['hr_samples'][idx] - esrgan_metrics['sr_samples'][idx])\n",
    "        plt.imshow(np.mean(error, axis=-1), cmap='inferno')\n",
    "        plt.title('Pixel Error Heatmap')\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ====================== EXECUTION FLOW ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load new evaluation dataset (corrected)\n",
    "    print(\"Loading new evaluation dataset...\")\n",
    "    new_lr, new_hr = load_dataset(DATASET_PATHS['new_dataset']['lr'],\n",
    "                                   DATASET_PATHS['new_dataset']['hr'])\n",
    "    \n",
    "    # 2. Evaluate ESRGAN model on dataset\n",
    "    print(\"\\nEvaluating on new dataset...\")\n",
    "    esrgan_metrics = evaluate_model(generator, new_lr, new_hr)\n",
    "    \n",
    "    # 3. Visual comparison with ESRGAN\n",
    "    print(\"\\nGenerating ESRGAN visual comparisons...\")\n",
    "    plot_esrgan_samples(esrgan_metrics)\n",
    "    \n",
    "    # 4. Save results\n",
    "    esrgan_metrics_df = pd.DataFrame({\n",
    "        'PSNR': esrgan_metrics['psnr'],\n",
    "        'SSIM': esrgan_metrics['ssim'],\n",
    "        'MSE': esrgan_metrics['mse']\n",
    "    })\n",
    "    esrgan_metrics_df.to_csv('esrgan_comparison.csv', index=False)\n",
    "    print(\"\\nComparison saved to esrgan_comparison.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10257cd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a98c39",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09030c0d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6590540,
     "sourceId": 10643951,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6631100,
     "sourceId": 10700442,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32813.239104,
   "end_time": "2025-02-12T02:12:05.651484",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-11T17:05:12.412380",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
